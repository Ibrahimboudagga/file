# -*- coding: utf-8 -*-
"""Case_study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L-nhvOGa2MoSzkRuu3R9zBNwGagSmqlV

Let's import some libraries
"""

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt 
import os
import seaborn as sns
from math import isnan

"""Let's explore the data

"""

!git clone https://github.com/Ibrahimboudagga/file.git

!pip install pyspark 
#!pip install tensorflow-data-validation

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('aa').getOrCreate()

data = spark.read.csv('/content/file/ATUCE_-_Data_Scientist_-_Case_study_data_2022_02.csv', header = True, sep = ';')
data.createOrReplaceTempView('data')
df = spark.sql('SELECT * FROM data')
df =df.toPandas()

df.head()

"""we have to substitute all '�' by the approriate value basing on the vlues in the origin csv file """

for col in df.columns:
  qol =col
  qol = qol.replace('�', 'é')
  df.rename(columns={col: qol}, inplace = True)

for i in df.index:
  a =df['Montant'][i]
  a =str(a).replace('�', ' ')
  df['Montant'][i] = str(a)

for i in df.index:
  a =df['capital social'][i]
  a =str(a).replace('�', ' ')
  df['capital social'][i] = str(a)

for i in df.index:
  a =df['Emprunteur'][i]
  a =str(a).replace('�', 'é')
  df['Emprunteur'][i] = str(a)

for i in df.index:
  a =df['effectifs'][i]
  a =str(a).replace('�', 'à')
  df['effectifs'][i] = str(a)

"""we have to eliminate all 'symbols' character like '-,+,%,*'"""

for col in df.columns:
  for i in df.index :
    df[col][i] = str(df[col][i]).lower()
    df[col][i] = str(df[col][i]).replace('%', '')
    df[col][i] = str(df[col][i]).replace('*', '')
    df[col][i] = str(df[col][i]).replace('+', '')
    if len(str(df[col][i])) ==1 :
      df[col][i] = str(df[col][i]).replace('-', '')

for col in df.columns:
  for i in df.index :
    
    df[col][i] = str(df[col][i]).replace(',', '.')

"""it's oligatory to convert our data from string to float in order to make some mathematics operations"""

for col in df.columns:
  for i in df.index:
    msg = df[col][i].isnumeric()
    msg2 = df[col][i].islower()
    if msg2 == False:
        a = df[col][i].replace(' ', "")
        df[col][i] =a
    if msg == True:
      df[col][i] = float(df[col][i])
s = 0
for col in df.columns:
  for i in df.index :
    if str(df[col][i]) == '' or str(df[col][i]) == 'none' or str(df[col][i])== 'n/a' or str(df[col][i])== '--' or str(df[col][i])== '-':
      df[col][i] = None
      s = s+1 
        
for col in df.columns : 
  for i in df.index :
    df[col][i] = str(df[col][i]).replace('�', '')
    if df[col][i] == 'None' or df[col][i] == 'n/a' :
      df[col][i] = None
for col in df.columns :
  for i in df.index : 
  
    if df[col][i]=='None' or df[col][i]== 'n/a ' or df[col][i] =='--' : 
      df[col][i] = None  

for col in df.columns:
  for i in df.index : 
    if df[col][i]== "na" or df[col][i]== "nan" : 
      df[col][i]= None
for col in df.columns:
  for i in df.index : 
    if df[col][i]=='' :
      df[col][i]= None

for col in df.columns :
  for i in df.index : 
  
    if df[col][i]=='None' or df[col][i]== 'n/a ' or df[col][i] =='--' : 
      df[col][i] = None

ind = []
for i in df.index : 
  if df['Pays'][i] == None:
    ind.append(i)
    
  
df = df.drop(ind, axis = 0)

df.shape

s=0
cols = {}
for col in df.columns:
  for i in df.index:
    if df[col][i] == 'none' or df[col][i] == None or df[col][i] =='n/a ' :
      s=s+1
  if s > (df.shape[0]*0.5) : 
   cols[col] = s
  s=0
cols

"""we displayed above all columns which the majority of their values is None.
let's explore this via a plot
"""

def pie_plot(dictionary):
  labels = ['None', 'other']
  i=1
  fig = plt.gcf()
  fig.set_size_inches(20, 20)
  for col in dictionary.keys():

    values = [dictionary[col], df.shape[0] - cols[col]]
    ax = plt.subplot(14,4, i)
    ax.pie(values, labels = labels,startangle =0, shadow = True ,autopct='%1.1f%%',  textprops ={'fontsize': 14} )
    plt.title(col, fontsize = 15)

    i=i+1 
  plt.subplots_adjust(left = 0, right = 0.8, bottom = 0, top = 2, wspace = 0.2, hspace = 0.2)
  return plt.show()

pie_plot(cols)

"""in this case the best method to get ride of these none values is to drop the entire column

"""

df = df.drop(columns= list(cols.keys()), axis = 1)

nv = {}
s =0 
for i in df.index:
  for col in df.columns : 
    if df[col][i] == None:
      s= s+1 
  if s >= (df.shape[1]*0.5) :
    nv[i]=s
  s=0

df=df.drop(list(nv.keys()), axis = 0, )

"""we deleted all rows that their half are filled by None value

by making a shallow analysis we can say that some feature hasn't any correlation or any effect on our target value 'Taux'
"""

df =  df.drop(columns = ['ID','Pays', 'Emprunteur', 'année de création', 'effectifs' ], axis = 1)

"""basing on the data sheet we found that the values in the columns ['EBE(retraité des loyers de leasing) 15',
       'EBE(retraité des loyers de leasing) 16'] aren't homogeneous for example one third of the valus in these columns are under the form of interval (50% - 55%/0% - 5%/20% - 25%.....) whereas the rest are just numbers (343/59/1739...), in fact the best way to avoid all kind of misleading i decide to delete these columns
"""

df = df.drop(columns =  ['EBE(retraité des loyers de leasing) 15', 'EBE(retraité des loyers de leasing) 16'], axis  = 1)

rn15 = {'0-5' : 2.5, '5-10': 7.5, '10-15': 12.5, '20-25': 22.5, '155-160': 157.5,'45-50': 47.5  }
df["Resultat Net 15"] =df["Resultat Net 15"].map(rn15)
rn16 = {'0-5': 2.5, '10-15': 12.5, '30-35': 32.5, '40-45': 42.5, '25-30': 27.5, '20-25':22.5, '100-105': 102.5, '155-160': 157.5, '125-130': 127.5}
df["Resultat Net 16"] =df["Resultat Net 16"].map(rn16)

from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
df['Niveau de risque'] = enc.fit_transform(df[['Niveau de risque']])
df = df.astype('float64')

for i in df.index:
  if df['Taux'][i] >= 1 :
   df['Taux'][i] = df['Taux'][i]/100

cols = df.columns
cols = cols.drop(["Resultat Net 15",	"Resultat Net 16"])

df.isnull().sum().sort_values()

df = df.astype('float64')

#qui  : to retrieve quantiles 
#inti  : to retrieve the cardinal of evry inerval
#indi  : to retrieve all row indexs that belong to the same intervall

a =df["Taux"]
qu1 = a.describe()['min']
qu2 = a.describe()['25%']
qu3 = a.describe()['50%']
qu4 = a.describe()['75%']
qu5 = a.describe()['max']
int1 =0
ind1 = []
int2 =0
ind2 = []
int3 =0
ind3 = []
int4 =0
ind4 = []

#we will devide de 'Taux' column to 4 intervals as bellow 
for i in df.index : 
  
  if df['Taux'][i]>= qu1 and df['Taux'][i]< qu2 : 
    int1 = int1+1
    ind1.append(i)
  if df['Taux'][i]>= qu2 and df['Taux'][i]< qu3 : 
    int2 = int1+2
    ind2.append(i)
  if df['Taux'][i]>= qu3 and df['Taux'][i]< qu4 : 
    int3 = int3+1
    ind3.append(i)
  if df['Taux'][i]>= qu4 and df['Taux'][i]<= qu5 : 
    int4 = int4+1
    ind4.append(i)

"""this data contains many null values even some column cantains more than 100 null values.
so to fix this issu, we will fill all these null values by the mean of a specific intervals
To fill all the null values using the said method is better then using the same mean of the entire columns.
"""

def fill_missing(col):
    a= df[col]
    rv1 = []
    rv2 = []
    rv3 = []
    rv4 = []
    nan = float('nan')
    for i in ind1 : 
      if isnan(a[i]) == False : 
        rv1.append(a[i])
    for i in ind2 : 
      if isnan(a[i]) == False : 
        rv2.append(a[i])
    for i in ind3 : 
      if isnan(a[i]) == False : 
        rv3.append(a[i])
    for i in ind4 : 
      if isnan(a[i]) == False : 
        rv4.append(a[i])

      

    meanind1 = sum(rv1)/len(rv1)
    meanind2 = sum(rv2)/len(rv2)
    meanind3 = sum(rv3)/len(rv3)
    meanind4 = sum(rv4)/len(rv4)

    for i in ind1 : 
      if isnan(a[i]) == True : 
        a[i] = meanind1
        
    for i in ind2 : 
      if isnan(a[i]) == True :  
        a[i] = meanind2
    for i in ind3 : 
      if isnan(a[i]) == True :  
        a[i] = meanind3
    for i in ind4 : 
      if isnan(a[i]) == True :  
        a[i] = meanind4
    return a

for col in df.columns:
  try:
      df[col]=fill_missing(col)
      

  except :
      print(col)

df.isnull().sum()

"""now  we will make some data visualisation to display correlation between data
 
"""

plt.figure(figsize = (20,10))
plt.title('heatmap')
sns.heatmap(df.corr(),linewidths=0.5,vmax=1.0, 
            square=True,  linecolor='white', annot=True)

"""we will focus just on the correlation between our target and the other features

"""

cor =  dict(abs(df.corr()['Taux'])) 
cor

"""the dictionary above show us that the correlation of  some columns is very weak (<0.1). So if you want you can incomment the code bellow to drop all columns with weak correlation """

#for col in cor.keys():
 # if cor[col] < 0.1 :
  #  df = df.drop(columns = [col], axis =0)

#cor =  dict(abs(df.corr()['Taux'])) 
#cor

"""for the optimization of resource, we will reduce the dimention of our data using the PCA methode then we will check the results """

from sklearn.decomposition import PCA

# Instantiate PCA without specifying number of components
pca_all = PCA()

# Fit to scaled data
pca_all.fit(x)

# Save cumulative explained variance
cum_var = (np.cumsum(pca_all.explained_variance_ratio_))
n_comp = [i for i in range(1, pca_all.n_components_ + 1)]

# Plot cumulative variance
ax = sns.pointplot(x=n_comp, y=cum_var)
ax.set(xlabel='number of  principal components', ylabel='cumulative explained variance')
plt.show()

"""the plot show us that just 8 component are enough to present the cumulativ variance """

#y1 = y
#x1 =x 
#pca10 = PCA(10)
#x1 = pca10.fit_transform(x1)

#x1 = pd.DataFrame(x1, columns = ['a', 'b', 'c', 'd', 'e', 'f','g','h','i', 'j'])

"""the data is very noisy, so the use of dimension reduction methods will end with no benifits """

def dist_plot(df):
  
  i=1
  fig = plt.gcf()
  fig.set_size_inches(20, 20)
  for col in df.columns:

    
    ax = plt.subplot(10,4, i)
    sns.distplot(df[col])
    plt.title(col, fontsize = 15)

    i=i+1 
  plt.subplots_adjust(left = 0, right = 0.8, bottom = 0, top = 2, wspace = 0.2, hspace = 0.2)
  return plt.show()

dist_plot(df)

"""all these charts chow us that our data is very noisy

the function bellow detect all outliers
"""

#we will compare the mean with the median
def stat_describ(b):
  a = b 
  a = a.dropna()
  val = {'(mean-median)':abs(a.mean()-a.median()),
      
      'mean': a.mean(),
      'median': a.median()
  }
  return val
stat = {}
for col in df.columns : 
  stat[col]= stat_describ(df[col])

stat

"""to detect all the ouliers values, we fall back on the method of  Z score = (Observation — Mean)/Standard Deviation
z = (X — μ) / σ
"""

from scipy import stats
def outliers_dtc(col):
  t = 3 #threshomd
  out_indx = []
  
  for i in df.index : 
    z = abs((df[col][i]-df[col].mean())/df[col].std())
    if z>t:
      out_indx.append(i)

  return out_indx

col_out = {}
for col in df.columns :
 col_out[col]=outliers_dtc(col)
 
col_out

out_row = []
for col in col_out.keys():
  for i in col_out[col]:
    out_row.append(i)

out_row = list(dict.fromkeys(out_row))
df = df.drop(out_row, axis = 0)

y = df['Taux']
x = df.drop(columns = ['Taux'], axis = 1)

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size =0.2, shuffle = True)

"""we will check the accuracy of testing and training of two different regression model using different values of hyperparameters """

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
o = 0


dict1 ={'n_estimators': [50,100,1000], 'max_depth': [5,10,100], 'n_jobs': [-1,1]}
eval_model = pd.DataFrame(columns = ['n_estimators', 'max_depth', 'njobs', 'accuracy' ])
for i in dict1['n_estimators'] :
  for j in dict1['max_depth']:
    for k in dict1['n_jobs']:
      model1 =RandomForestRegressor(n_estimators =i, max_depth = j, n_jobs=k )
      model1.fit(x_train, y_train)
      test_score = model1.score(x_test, y_test)
      train_score = model1.score(x_train, y_train)
      temp = pd.DataFrame( {'n_estimators': [i], 'max_depth': [j], 'njobs':[k], 'test_accuracy':[test_score], 'train_accuracy':[train_score]   })
      if o == 0 :
        eval_model = temp
      else: 
        eval_model = pd.concat([eval_model, temp], ignore_index = True, axis = 0)
      o= o+1 



eval_model['model'] = 'RandomForestRegressor'

#param = [{'n_estimators': [50,100,1000], 'max_depth': [5,10,10], 'n_jobs': [-1,1]},{'loss': ['squared_error', 'quantile'], 'learning_rate': [0.001,0.0001,0.01],'n_estimators': [50,100,1000]}, {'n_jobs': [-1,1]} ]

eval_model.sort_values('test_accuracy', ascending= True)

dict2 = {'loss': ['squared_error', 'quantile'], 'learning_rate': [0.001,0.0001,0.01],'n_estimators': [50,100,1000]}
eval_model1 = pd.DataFrame(columns = ['loss', 'learning_rate', 'n_estimators', 'accuracy' ])

for i in dict2['n_estimators'] :
  for j in dict2['loss']:
    for k in dict2['learning_rate']:
      
      model2 =GradientBoostingRegressor(loss = j, learning_rate=k, n_estimators=i)
      model2.fit(x_train, y_train)
      train_score = model2.score(x_train, y_train)
      test_score = model2.score(x_test, y_test)
      temp = pd.DataFrame( {'n_estimators': [i], 'learning_rate': [k], 'loss':[j], 'test_accuracy':[test_score], 'train_accuracy':[train_score]   })
      if o == 0 :
        eval_model1 = temp
      else: 
        eval_model1 = pd.concat([eval_model1, temp], ignore_index = True, axis = 0)
      o= o+1 
  
eval_model1['model'] = 'GradientBoostingRegressor'

eval_model1.sort_values('test_accuracy', ascending= True)

for i  in eval_model.index :
  if eval_model1['test_accuracy'][i] < eval_model['test_accuracy'].max() : 
    max_ind = i

for i  in eval_model1.index :
  if eval_model1['test_accuracy'][i]> eval_model['test_accuracy'].max() : 
    max_ind1 = i

best_model = pd.DataFrame()
if eval_model1['test_accuracy'].max() < eval_model['test_accuracy'].max() :
  best_model = eval_model[:][max_ind: max_ind+1]
else :
  best_model = eval_model1[:][max_ind1: max_ind1+1]

best_model

"""so now we can conclude that the the best model is RandomForestRegressor and the best parameters value are illustrated in the dataframe above."""

from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score
model =RandomForestRegressor(n_estimators =100, max_depth = 10, n_jobs=-1 )

stf = KFold(n_splits=20)
cross_val_score(model, x,y , cv =stf)

model =RandomForestRegressor(n_estimators =100, max_depth = 10, n_jobs=-1 )
model.fit(x_train, y_train)
model.score(x_test, y_test)

